# Ollama Configuration
# Copy this file to .env and adjust values as needed

# Ollama model name (default: llama2)
# Popular options: llama2, mistral, codellama, phi, neural-chat
OLLAMA_MODEL=llama2

# Ollama server base URL (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434
